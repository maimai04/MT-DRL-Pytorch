{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "from config.config import *\n",
    "#from analysis.analysis_functions import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Andy\\\\PycharmProjects\\\\finrlpaper2\\\\MT-DRL-Pytorch'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abspath = r\"C:\\Users\\Andy\\PycharmProjects\\finrlpaper2\\MT-DRL-Pytorch\"\n",
    "os.chdir(abspath)\n",
    "cwd_ = os.getcwd() # get current working directory\n",
    "cwd_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results_path = \"results\"\n",
    "run_path = os.path.join(results_path, \"07-06-2021_17-43-02_ppoCustomBase_fm2_st\")\n",
    "seed = 0\n",
    "seed_path = os.path.join(run_path, \"agentSeed0\")\n",
    "\n",
    "# paths to DATAFILE\n",
    "data_path = os.path.join(abspath, \"data\", \"preprocessed\", \"US_stocks_WDB.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "# mode\n",
    "def get_results_dict_for_one_seed(seed_path: str, \n",
    "                                   backtest_path: str, \n",
    "                                   mode=\"test\", \n",
    "                                  ):\n",
    "    # performance paths for FOLDERS\n",
    "    pfvalue_path = os.path.join(seed_path, \"portfolio_value\")\n",
    "    reward_path = os.path.join(seed_path, \"rewards\")\n",
    "    all_weights_path = os.path.join(seed_path, \"all_weights_cashAtEnd\")\n",
    "    equity_weights_path = os.path.join(seed_path, \"asset_equity_weights\")\n",
    "    policy_actions_path = os.path.join(seed_path, \"policy_actions\")\n",
    "    exer_actions_path = os.path.join(seed_path, \"exercised_actions\")\n",
    "    state_mem_path = os.path.join(seed_path, \"state_memory\")\n",
    "    # training performance\n",
    "    training_performance_path = os.path.join(seed_path, \"training_performance\")\n",
    "    # for backtesting\n",
    "    backtest_path = os.path.join(seed_path, \"backtest\")\n",
    "    bt_pfvalue_path = os.path.join(backtest_path, \"portfolio_value\")\n",
    "    bt_reward_path = os.path.join(backtest_path, \"rewards\")\n",
    "    bt_all_weights_path = os.path.join(backtest_path, \"all_weights_cashAtEnd\")\n",
    "    bt_equity_weights_path = os.path.join(backtest_path, \"asset_equity_weights\")\n",
    "    bt_policy_actions_path = os.path.join(backtest_path, \"policy_actions\")\n",
    "    bt_exer_actions_path = os.path.join(backtest_path, \"exercised_actions\")\n",
    "    bt_state_mem_path = os.path.join(backtest_path, \"state_memory\")\n",
    "    # create dictionarys with results paths of the folders aggregated\n",
    "    results_dict = {\"pfvalue\": glob.glob(os.path.join(pfvalue_path, f\"*{mode}*.csv\")),\n",
    "                   \"reward\": glob.glob(os.path.join(reward_path, f\"*{mode}*.csv\")),\n",
    "                   \"all_weights\": glob.glob(os.path.join(all_weights_path, f\"*{mode}*.csv\")),\n",
    "                   \"equity_weights\": glob.glob(os.path.join(equity_weights_path, f\"*{mode}*.csv\")),\n",
    "                   \"policy_actions\": glob.glob(os.path.join(policy_actions_path, f\"*{mode}*.csv\")),\n",
    "                   \"exer_actions\": glob.glob(os.path.join(exer_actions_path, f\"*{mode}*.csv\")),\n",
    "                   \"state_mem\": glob.glob(os.path.join(state_mem_path, f\"*{mode}*.csv\")),\n",
    "                   }\n",
    "    backtest_dict = {\"pfvalue\": glob.glob(os.path.join(bt_pfvalue_path, f\"*{mode}*.csv\")),\n",
    "                   \"reward\": glob.glob(os.path.join(bt_reward_path, f\"*{mode}*.csv\")),\n",
    "                   \"all_weights\": glob.glob(os.path.join(bt_all_weights_path, f\"*{mode}*.csv\")),\n",
    "                   \"equity_weights\": glob.glob(os.path.join(bt_equity_weights_path, f\"*{mode}*.csv\")),\n",
    "                   \"policy_actions\": glob.glob(os.path.join(bt_policy_actions_path, f\"*{mode}*.csv\")),\n",
    "                   \"exer_actions\": glob.glob(os.path.join(bt_exer_actions_path, f\"*{mode}*.csv\")),\n",
    "                   \"state_mem\": glob.glob(os.path.join(bt_state_mem_path, f\"*{mode}*.csv\")),\n",
    "                   }\n",
    "    state_header_df = pd.read_csv(os.path.join(state_mem_path, \"state_header.csv\"), index_col=0)\n",
    "    # get the data from the paths saved in the dictionary\n",
    "    ### FOR RESULTS DICTIONARY\n",
    "    results_dicty = results_dict.copy() \n",
    "    for key in results_dicty:\n",
    "        # for every key in results_dicty (e.g. \"pfvalue\", \"reward\",...) we get the list of filepaths\n",
    "        # (there are multiple filepaths since we have multiple episodes for which we needed to save results\n",
    "        # and we want to concatenate these episodes into one time series to get the overall result\n",
    "        filepaths = results_dicty[key]\n",
    "        # create empty list\n",
    "        li = []\n",
    "        # for each filepath, we read in the csv file as pandas dataframe and then append the df to the list\n",
    "        for file in filepaths:\n",
    "            df = pd.read_csv(file, index_col=0)\n",
    "            li.append(df)\n",
    "        # finally, we concatenate the df's in the list to one dataframe (concatenate on index axis \n",
    "        # => below each other, since they build a time series)\n",
    "        df = pd.concat(li)\n",
    "        # rename the first column, which is always \"datadate\" in our results files\n",
    "        df.rename(columns={df.columns[0]: \"datadate\"}, inplace = True)\n",
    "        # rename the one other column for rewards and portfolio value using the respective key\n",
    "        if key in [\"pfvalue\", \"reward\"]:\n",
    "            df.rename(columns={df.columns[1]: key}, inplace = True)\n",
    "        # for the state memory, we use the state header to as column names\n",
    "        if key == \"state_mem\":\n",
    "            df.columns = [\"datadate\"] + state_header_df.values.flatten().tolist()\n",
    "        # sort based on date (since we want to have a nice time series and \"glob\" does not\n",
    "        # necessarily import the fileüaths in the correct order)\n",
    "        df = df.sort_values(\"datadate\")\n",
    "        # drop duplicate values (usually, the last state (where no action done anymore) is still saved in the episode results file,\n",
    "        # and at the same time, it is saved in the results file of the next episode as the \"initial\" state, where we do an action.\n",
    "        # This is not wrong (it is actuall practical to debug and check if the cirrect starting state is used in the episodes),\n",
    "        # but we don't want to have it double here for time series analysis (wouldn't make sense))\n",
    "        df = df.drop_duplicates(subset=[\"datadate\"], keep='last')\n",
    "        # include the results in the dictionary\n",
    "        results_dicty.update({key: df})\n",
    "    ### FOR BACKTESTING DICTIONARY\n",
    "    backtest_dicty = results_dict.copy() \n",
    "    for key in backtest_dicty:\n",
    "        # for every key in results_dicty (e.g. \"pfvalue\", \"reward\",...) we get the list of filepaths\n",
    "        # (there are multiple filepaths since we have multiple episodes for which we needed to save results\n",
    "        # and we want to concatenate these episodes into one time series to get the overall result\n",
    "        filepaths = backtest_dicty[key]\n",
    "        # create empty list\n",
    "        li = []\n",
    "        # for each filepath, we read in the csv file as pandas dataframe and then append the df to the list\n",
    "        for file in filepaths:\n",
    "            df = pd.read_csv(file, index_col=0)\n",
    "            li.append(df)\n",
    "        # finally, we concatenate the df's in the list to one dataframe (concatenate on index axis \n",
    "        # => below each other, since they build a time series)\n",
    "        df = pd.concat(li)\n",
    "        # rename the first column, which is always \"datadate\" in our results files\n",
    "        df.rename(columns={df.columns[0]: \"datadate\"}, inplace = True)\n",
    "        # rename the one other column for rewards and portfolio value using the respective key\n",
    "        if key in [\"pfvalue\", \"reward\"]:\n",
    "            df.rename(columns={df.columns[1]: key}, inplace = True)\n",
    "        # for the state memory, we use the state header to as column names\n",
    "        if key == \"state_mem\":\n",
    "            df.columns = [\"datadate\"] + state_header_df.values.flatten().tolist()\n",
    "        # sort based on date (since we want to have a nice time series and \"glob\" does not\n",
    "        # necessarily import the fileüaths in the correct order)\n",
    "        df = df.sort_values(\"datadate\")\n",
    "        # drop duplicate values (usually, the last state (where no action done anymore) is still saved in the episode results file,\n",
    "        # and at the same time, it is saved in the results file of the next episode as the \"initial\" state, where we do an action.\n",
    "        # This is not wrong (it is actuall practical to debug and check if the cirrect starting state is used in the episodes),\n",
    "        # but we don't want to have it double here for time series analysis (wouldn't make sense))\n",
    "        df = df.drop_duplicates(subset=[\"datadate\"], keep='last')\n",
    "        # include the results in the dictionary\n",
    "        backtest_dicty.update({key: df})\n",
    "\n",
    "    # the last three outputs are optional, just used for debugging\n",
    "    return results_dicty, backtest_dicty,  state_header_df, results_dict, backtest_dict \n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict, backtest_dict, _, _, _ = get_results_dict_for_one_seed(seed_path=seed_path, \n",
    "                                                                              backtest_path=backtest_path,\n",
    "                                                                              mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "def calculate_and_save_performance_metrics(results_dict: dict, save_path: str, seed: int=None, mode: str=\"test\"):\n",
    "    import ffn\n",
    "    ### CALCULATE\n",
    "    # sharpe ratio, max DD, average DD, total ret, USING RISK.FREE RATE OF 0\n",
    "    # first, we need to convert datadate from integer to datetime format, so the library (ffn) an work with it\n",
    "    results_dict[\"pfvalue\"][\"datadate\"] =  pd.to_datetime(results_dict[\"pfvalue\"][\"datadate\"], format='%Y%m%d')\n",
    "    # then we can create a \"perf\" object (performances) with the function .calc_stats()\n",
    "    perf = results_dict[\"pfvalue\"].set_index(\"datadate\")[\"pfvalue\"].calc_stats()\n",
    "    # now we can acces sthe statistics like this, for example: (ann = annuaized)\n",
    "    sharpe_ratio_daily_ann = perf.daily_sharpe\n",
    "    total_return = perf.total_return\n",
    "    avg_daily_return_ann = perf.daily_mean\n",
    "    std_daily_return_ann = perf.daily_vol\n",
    "    maxdd = perf.max_drawdown\n",
    "    avg_dd = perf.avg_drawdown\n",
    "    avg_dd_days = perf.avg_drawdown_days\n",
    "    # calculate cumulative return\n",
    "    cumret =  results_dict[\"pfvalue\"][\"pfvalue\"]\n",
    "    \n",
    "    ### SAVE\n",
    "    df = pd.DataFrame({\"performance_metric\": \n",
    "                       [\"sharpe_ratio_daily_ann\", \"total_return\", \"avg_daily_return_ann\", \"std_daily_return_ann\",\n",
    "                        \"maxdd\", \"avg_dd\", \"avg_dd_days\" ], \n",
    "                       f\"seed{seed}\": \n",
    "                       [sharpe_ratio_daily_ann, total_return, avg_daily_return_ann, std_daily_return_ann, \n",
    "                        maxdd, avg_dd, avg_dd_days]})\n",
    "    df.to_csv(os.path.join(save_path, f\"{mode}_performance_metrics_seed{seed}.csv\"))\n",
    "    return None\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_and_save_performance_metrics(results_dict=results_dict, \n",
    "                                       save_path=seed_path, \n",
    "                                       seed=seed,\n",
    "                                       mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datadate</th>\n",
       "      <th>PF_Value</th>\n",
       "      <th>cum_return_daily</th>\n",
       "      <th>return_daily</th>\n",
       "      <th>log_return_daily</th>\n",
       "      <th>cum_logreturn_daily</th>\n",
       "      <th>cum_return_daily2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2021-06-07</td>\n",
       "      <td>1.969737e+06</td>\n",
       "      <td>1.969737</td>\n",
       "      <td>-0.006111</td>\n",
       "      <td>-0.006129</td>\n",
       "      <td>0.677900</td>\n",
       "      <td>1.969737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2021-06-08</td>\n",
       "      <td>1.962651e+06</td>\n",
       "      <td>1.962651</td>\n",
       "      <td>-0.003598</td>\n",
       "      <td>-0.003604</td>\n",
       "      <td>0.674296</td>\n",
       "      <td>1.962651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2021-06-09</td>\n",
       "      <td>1.958215e+06</td>\n",
       "      <td>1.958215</td>\n",
       "      <td>-0.002260</td>\n",
       "      <td>-0.002263</td>\n",
       "      <td>0.672033</td>\n",
       "      <td>1.958215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2021-06-10</td>\n",
       "      <td>1.935486e+06</td>\n",
       "      <td>1.935486</td>\n",
       "      <td>-0.011607</td>\n",
       "      <td>-0.011675</td>\n",
       "      <td>0.660358</td>\n",
       "      <td>1.935486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2021-06-11</td>\n",
       "      <td>1.945899e+06</td>\n",
       "      <td>1.945899</td>\n",
       "      <td>0.005380</td>\n",
       "      <td>0.005366</td>\n",
       "      <td>0.665724</td>\n",
       "      <td>1.945899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     datadate      PF_Value  cum_return_daily  return_daily  log_return_daily  \\\n",
       "43 2021-06-07  1.969737e+06          1.969737     -0.006111         -0.006129   \n",
       "44 2021-06-08  1.962651e+06          1.962651     -0.003598         -0.003604   \n",
       "45 2021-06-09  1.958215e+06          1.958215     -0.002260         -0.002263   \n",
       "46 2021-06-10  1.935486e+06          1.935486     -0.011607         -0.011675   \n",
       "47 2021-06-11  1.945899e+06          1.945899      0.005380          0.005366   \n",
       "\n",
       "    cum_logreturn_daily  cum_return_daily2  \n",
       "43             0.677900           1.969737  \n",
       "44             0.674296           1.962651  \n",
       "45             0.672033           1.958215  \n",
       "46             0.660358           1.935486  \n",
       "47             0.665724           1.945899  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dicty[\"pfvalue\"][\"return_daily\"] = results_dicty[\"pfvalue\"][\"PF_Value\"].pct_change()\n",
    "results_dicty[\"pfvalue\"]['log_return_daily'] = np.log(\n",
    "    results_dicty[\"pfvalue\"][\"PF_Value\"]) - np.log(results_dicty[\"pfvalue\"][\"PF_Value\"].shift(1))\n",
    "results_dicty[\"pfvalue\"]['cum_return_daily2'] = np.exp(np.log1p(results_dicty[\"pfvalue\"]['return_daily']).cumsum())\n",
    "results_dicty[\"pfvalue\"]['cum_logreturn_daily'] = results_dicty[\"pfvalue\"]['log_return_daily'].cumsum()\n",
    "\n",
    "results_dicty[\"pfvalue\"].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9458994134600001\n",
      "0.15294309075252352\n",
      "0.2213404189979843\n",
      "0.6909858192412491\n"
     ]
    }
   ],
   "source": [
    "# Sharpe Ratio Annualized\n",
    "# note: daily yield curve, 1year, has been going down from ~0.5% (2009) to 0.05% (2021), \n",
    "# and here the test period is only from 2016 on,\n",
    "# so might consider 0 for simplicity or an average\n",
    "# for now, 0\n",
    "# see also: https://quant.stackexchange.com/questions/28385/what-value-should-the-risk-free-monthly-return-rate-be-sharpe-ratio-calculation\n",
    "# and: https://www.treasury.gov/resource-center/data-chart-center/interest-rates/Pages/TextView.aspx?data=yield\n",
    "\n",
    "avg_daily_return_ann = 252 * results_dicty[\"pfvalue\"][\"return_daily\"].mean() # assuming 252 trading days\n",
    "std_daily_return_ann = np.sqrt(252) * results_dicty[\"pfvalue\"][\"return_daily\"].std() # assuming 252 trading days\n",
    "sharpe_ratio_ann =  avg_daily_return_ann/std_daily_return_ann\n",
    "total_return = results_dicty[\"pfvalue\"][\"PF_Value\"].iloc[-1] / results_dicty[\"pfvalue\"][\"PF_Value\"].iloc[0] -1\n",
    "\n",
    "print(total_return) \n",
    "print(avg_daily_return_ann)\n",
    "print(std_daily_return_ann)\n",
    "print(sharpe_ratio_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict[\"pfvalue\"][\"datadate\"] =  pd.to_datetime(results_dict[\"pfvalue\"][\"datadate\"], format='%Y%m%d')\n",
    "perf = results_dict[\"pfvalue\"].set_index(\"datadate\")[\"pfvalue\"].calc_stats()\n",
    "\n",
    "\n",
    "# daily return:\n",
    "df['daily_return'] = df['close'].pct_change()\n",
    "# calculate cumluative return\n",
    "df['cumluative_return'] = np.exp(np.log1p(df['daily_return']).cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'pfvalue Price Series'}, xlabel='datadate'>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf.plot()\n",
    "perf.display()\n",
    "perf.plot_histogram()\n",
    "perf.display_monthly_returns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUMULATIVE RETURNS PLOT OF PORTFOLIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
