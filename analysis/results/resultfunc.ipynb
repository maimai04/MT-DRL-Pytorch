{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "from config.config import *\n",
    "#from analysis.analysis_functions import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Andy\\\\PycharmProjects\\\\finrlpaper2\\\\MT-DRL-Pytorch'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abspath = r\"C:\\Users\\Andy\\PycharmProjects\\finrlpaper2\\MT-DRL-Pytorch\"\n",
    "os.chdir(abspath)\n",
    "cwd_ = os.getcwd() # get current working directory\n",
    "cwd_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results_path = \"results\"\n",
    "run_path = os.path.join(results_path, \"07-06-2021_17-43-02_ppoCustomBase_fm2_st\")\n",
    "seed = 0\n",
    "seed_path = os.path.join(run_path, \"agentSeed0\")\n",
    "\n",
    "# paths to DATAFILE\n",
    "data_path = os.path.join(abspath, \"data\", \"preprocessed\", \"US_stocks_WDB.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "# mode\n",
    "def get_results_dict_for_one_seed(seed_path: str, \n",
    "                                   backtest_path: str, \n",
    "                                   mode=\"test\", \n",
    "                                  ):\n",
    "    # performance paths for FOLDERS\n",
    "    pfvalue_path = os.path.join(seed_path, \"portfolio_value\")\n",
    "    reward_path = os.path.join(seed_path, \"rewards\")\n",
    "    all_weights_path = os.path.join(seed_path, \"all_weights_cashAtEnd\")\n",
    "    equity_weights_path = os.path.join(seed_path, \"asset_equity_weights\")\n",
    "    policy_actions_path = os.path.join(seed_path, \"policy_actions\")\n",
    "    exer_actions_path = os.path.join(seed_path, \"exercised_actions\")\n",
    "    state_mem_path = os.path.join(seed_path, \"state_memory\")\n",
    "    # training performance\n",
    "    training_performance_path = os.path.join(seed_path, \"training_performance\")\n",
    "    # for backtesting\n",
    "    backtest_path = os.path.join(seed_path, \"backtest\")\n",
    "    bt_pfvalue_path = os.path.join(backtest_path, \"portfolio_value\")\n",
    "    bt_reward_path = os.path.join(backtest_path, \"rewards\")\n",
    "    bt_all_weights_path = os.path.join(backtest_path, \"all_weights_cashAtEnd\")\n",
    "    bt_equity_weights_path = os.path.join(backtest_path, \"asset_equity_weights\")\n",
    "    bt_policy_actions_path = os.path.join(backtest_path, \"policy_actions\")\n",
    "    bt_exer_actions_path = os.path.join(backtest_path, \"exercised_actions\")\n",
    "    bt_state_mem_path = os.path.join(backtest_path, \"state_memory\")\n",
    "    # create dictionarys with results paths of the folders aggregated\n",
    "    results_dict = {\"pfvalue\": glob.glob(os.path.join(pfvalue_path, f\"*{mode}*.csv\")),\n",
    "                   \"reward\": glob.glob(os.path.join(reward_path, f\"*{mode}*.csv\")),\n",
    "                   \"all_weights\": glob.glob(os.path.join(all_weights_path, f\"*{mode}*.csv\")),\n",
    "                   \"equity_weights\": glob.glob(os.path.join(equity_weights_path, f\"*{mode}*.csv\")),\n",
    "                   \"policy_actions\": glob.glob(os.path.join(policy_actions_path, f\"*{mode}*.csv\")),\n",
    "                   \"exer_actions\": glob.glob(os.path.join(exer_actions_path, f\"*{mode}*.csv\")),\n",
    "                   \"state_mem\": glob.glob(os.path.join(state_mem_path, f\"*{mode}*.csv\")),\n",
    "                   }\n",
    "    backtest_dict = {\"pfvalue\": glob.glob(os.path.join(bt_pfvalue_path, f\"*{mode}*.csv\")),\n",
    "                   \"reward\": glob.glob(os.path.join(bt_reward_path, f\"*{mode}*.csv\")),\n",
    "                   \"all_weights\": glob.glob(os.path.join(bt_all_weights_path, f\"*{mode}*.csv\")),\n",
    "                   \"equity_weights\": glob.glob(os.path.join(bt_equity_weights_path, f\"*{mode}*.csv\")),\n",
    "                   \"policy_actions\": glob.glob(os.path.join(bt_policy_actions_path, f\"*{mode}*.csv\")),\n",
    "                   \"exer_actions\": glob.glob(os.path.join(bt_exer_actions_path, f\"*{mode}*.csv\")),\n",
    "                   \"state_mem\": glob.glob(os.path.join(bt_state_mem_path, f\"*{mode}*.csv\")),\n",
    "                   }\n",
    "    state_header_df = pd.read_csv(os.path.join(state_mem_path, \"state_header.csv\"), index_col=0)\n",
    "    # get the data from the paths saved in the dictionary\n",
    "    ### FOR RESULTS DICTIONARY\n",
    "    results_dicty = results_dict.copy() \n",
    "    for key in results_dicty:\n",
    "        # for every key in results_dicty (e.g. \"pfvalue\", \"reward\",...) we get the list of filepaths\n",
    "        # (there are multiple filepaths since we have multiple episodes for which we needed to save results\n",
    "        # and we want to concatenate these episodes into one time series to get the overall result\n",
    "        filepaths = results_dicty[key]\n",
    "        # create empty list\n",
    "        li = []\n",
    "        # for each filepath, we read in the csv file as pandas dataframe and then append the df to the list\n",
    "        for file in filepaths:\n",
    "            df = pd.read_csv(file, index_col=0)\n",
    "            li.append(df)\n",
    "        # finally, we concatenate the df's in the list to one dataframe (concatenate on index axis \n",
    "        # => below each other, since they build a time series)\n",
    "        df = pd.concat(li)\n",
    "        # rename the first column, which is always \"datadate\" in our results files\n",
    "        df.rename(columns={df.columns[0]: \"datadate\"}, inplace = True)\n",
    "        # rename the one other column for rewards and portfolio value using the respective key\n",
    "        if key in [\"pfvalue\", \"reward\"]:\n",
    "            df.rename(columns={df.columns[1]: key}, inplace = True)\n",
    "        # for the state memory, we use the state header to as column names\n",
    "        if key == \"state_mem\":\n",
    "            df.columns = [\"datadate\"] + state_header_df.values.flatten().tolist()\n",
    "        # sort based on date (since we want to have a nice time series and \"glob\" does not\n",
    "        # necessarily import the fileüaths in the correct order)\n",
    "        df = df.sort_values(\"datadate\")\n",
    "        # drop duplicate values (usually, the last state (where no action done anymore) is still saved in the episode results file,\n",
    "        # and at the same time, it is saved in the results file of the next episode as the \"initial\" state, where we do an action.\n",
    "        # This is not wrong (it is actuall practical to debug and check if the cirrect starting state is used in the episodes),\n",
    "        # but we don't want to have it double here for time series analysis (wouldn't make sense))\n",
    "        df = df.drop_duplicates(subset=[\"datadate\"], keep='last')\n",
    "        # include the results in the dictionary\n",
    "        results_dicty.update({key: df})\n",
    "    ### FOR BACKTESTING DICTIONARY\n",
    "    backtest_dicty = results_dict.copy() \n",
    "    for key in backtest_dicty:\n",
    "        # for every key in results_dicty (e.g. \"pfvalue\", \"reward\",...) we get the list of filepaths\n",
    "        # (there are multiple filepaths since we have multiple episodes for which we needed to save results\n",
    "        # and we want to concatenate these episodes into one time series to get the overall result\n",
    "        filepaths = backtest_dicty[key]\n",
    "        # create empty list\n",
    "        li = []\n",
    "        # for each filepath, we read in the csv file as pandas dataframe and then append the df to the list\n",
    "        for file in filepaths:\n",
    "            df = pd.read_csv(file, index_col=0)\n",
    "            li.append(df)\n",
    "        # finally, we concatenate the df's in the list to one dataframe (concatenate on index axis \n",
    "        # => below each other, since they build a time series)\n",
    "        df = pd.concat(li)\n",
    "        # rename the first column, which is always \"datadate\" in our results files\n",
    "        df.rename(columns={df.columns[0]: \"datadate\"}, inplace = True)\n",
    "        # rename the one other column for rewards and portfolio value using the respective key\n",
    "        if key in [\"pfvalue\", \"reward\"]:\n",
    "            df.rename(columns={df.columns[1]: key}, inplace = True)\n",
    "        # for the state memory, we use the state header to as column names\n",
    "        if key == \"state_mem\":\n",
    "            df.columns = [\"datadate\"] + state_header_df.values.flatten().tolist()\n",
    "        # sort based on date (since we want to have a nice time series and \"glob\" does not\n",
    "        # necessarily import the fileüaths in the correct order)\n",
    "        df = df.sort_values(\"datadate\")\n",
    "        # drop duplicate values (usually, the last state (where no action done anymore) is still saved in the episode results file,\n",
    "        # and at the same time, it is saved in the results file of the next episode as the \"initial\" state, where we do an action.\n",
    "        # This is not wrong (it is actuall practical to debug and check if the cirrect starting state is used in the episodes),\n",
    "        # but we don't want to have it double here for time series analysis (wouldn't make sense))\n",
    "        df = df.drop_duplicates(subset=[\"datadate\"], keep='last')\n",
    "        # include the results in the dictionary\n",
    "        backtest_dicty.update({key: df})\n",
    "\n",
    "    # the last three outputs are optional, just used for debugging\n",
    "    return results_dicty, backtest_dicty,  state_header_df, results_dict, backtest_dict \n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict, backtest_dict, _, _, _ = get_results_dict_for_one_seed(seed_path=seed_path, \n",
    "                                                                              backtest_path=backtest_path,\n",
    "                                                                              mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "def calculate_and_save_performance_metrics(results_dict: dict, save_path: str, seed: int=None, mode: str=\"test\"):\n",
    "    import ffn\n",
    "    ### CALCULATE\n",
    "    # sharpe ratio, max DD, average DD, total ret, USING RISK.FREE RATE OF 0\n",
    "    # first, we need to convert datadate from integer to datetime format, so the library (ffn) an work with it\n",
    "    results_dict[\"pfvalue\"][\"datadate\"] =  pd.to_datetime(results_dict[\"pfvalue\"][\"datadate\"], format='%Y%m%d')\n",
    "    # then we can create a \"perf\" object (performances) with the function .calc_stats()\n",
    "    perf = results_dict[\"pfvalue\"].set_index(\"datadate\")[\"pfvalue\"].calc_stats()\n",
    "    # now we can acces sthe statistics like this, for example: (ann = annuaized)\n",
    "    sharpe_ratio_daily_ann = perf.daily_sharpe\n",
    "    total_return = perf.total_return\n",
    "    avg_daily_return_ann = perf.daily_mean\n",
    "    std_daily_return_ann = perf.daily_vol\n",
    "    maxdd = perf.max_drawdown\n",
    "    avg_dd = perf.avg_drawdown\n",
    "    avg_dd_days = perf.avg_drawdown_days\n",
    "    # calculate cumulative return\n",
    "    cumret =  results_dict[\"pfvalue\"][\"pfvalue\"]\n",
    "    \n",
    "    ### SAVE\n",
    "    df = pd.DataFrame({\"performance_metric\": \n",
    "                       [\"sharpe_ratio_daily_ann\", \"total_return\", \"avg_daily_return_ann\", \"std_daily_return_ann\",\n",
    "                        \"maxdd\", \"avg_dd\", \"avg_dd_days\" ], \n",
    "                       f\"seed{seed}\": \n",
    "                       [sharpe_ratio_daily_ann, total_return, avg_daily_return_ann, std_daily_return_ann, \n",
    "                        maxdd, avg_dd, avg_dd_days]})\n",
    "    df.to_csv(os.path.join(save_path, f\"{mode}_performance_metrics_seed{seed}.csv\"))\n",
    "    return None\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_and_save_performance_metrics(results_dict=results_dict, \n",
    "                                       save_path=seed_path, \n",
    "                                       seed=seed,\n",
    "                                       mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dicty[\"pfvalue\"][\"return_daily\"] = results_dicty[\"pfvalue\"][\"PF_Value\"].pct_change()\n",
    "results_dicty[\"pfvalue\"]['log_return_daily'] = np.log(\n",
    "    results_dicty[\"pfvalue\"][\"PF_Value\"]) - np.log(results_dicty[\"pfvalue\"][\"PF_Value\"].shift(1))\n",
    "results_dicty[\"pfvalue\"]['cum_return_daily2'] = np.exp(np.log1p(results_dicty[\"pfvalue\"]['return_daily']).cumsum())\n",
    "results_dicty[\"pfvalue\"]['cum_logreturn_daily'] = results_dicty[\"pfvalue\"]['log_return_daily'].cumsum()\n",
    "\n",
    "results_dicty[\"pfvalue\"].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'pfvalue Price Series'}, xlabel='datadate'>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf.plot()\n",
    "perf.display()\n",
    "perf.plot_histogram()\n",
    "perf.display_monthly_returns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUMULATIVE RETURNS PLOT OF PORTFOLIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}